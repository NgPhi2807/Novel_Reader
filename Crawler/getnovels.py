import requests
from bs4 import BeautifulSoup
import json
import time
import re
from urllib.parse import urljoin
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

BASE_URL = 'https://metruyenchu.com.vn'
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}

# Danh s√°ch th·ªÉ lo·∫°i v√† ID t∆∞∆°ng ·ª©ng
category_mapping = {
    "Ti√™n Hi·ªáp": 16, "V√µng Du": 17, "Huy·ªÅn Huy·ªÖn": 18, "Qu√¢n S·ª±": 19, "Xuy√™n Nhanh": 20,
    "Linh D·ªã": 21, "S·ªßng": 22, "Gia ƒê·∫•u": 23, "ƒêi·ªÅn VƒÉn": 24, "N·ªØ Ph·ª•": 25, "Hi·ªán ƒê·∫°i": 26,
    "Ki·∫øm Hi·ªáp": 27, "Khoa Huy·ªÖn": 28, "D·ªã Gi·ªõi": 29, "L·ªãch S·ª≠": 30, "Tr·ªçng Sinh": 31,
    "Ng∆∞·ª£c": 32, "Cung ƒê·∫•u": 33, "ƒê√¥ng Ph∆∞∆°ng": 34, "M·∫°t Th·∫ø": 35, "Ng√¥n T√¨nh": 36,
    "Quan Tr∆∞·ªùng": 37, "H·ªá Th·ªëng": 38, "D·ªã NƒÉng": 39, "Xuy√™n Kh√¥ng": 40, "Trinh Th√°m": 41,
    "S·∫Øc": 42, "N·ªØ C∆∞·ªùng": 43, "ƒê√¥ Th·ªã": 44, "Truy·ªán Teen": 45, "ƒêo·∫£n VƒÉn": 46
}

def fetch_story_detail(story):
    url = story['url']
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        description_elem = soup.select_one('.intro')
        description = description_elem.get_text(separator=" ").strip() if description_elem else "Kh√¥ng c√≥ m√¥ t·∫£"

        img_elem = soup.select_one('.book-info-pic img')
        img_url = img_elem['src'] if img_elem else story['image_url']
        if img_url and not img_url.startswith(('http://', 'https://')):
            img_url = urljoin(BASE_URL, img_url)

        date_update = datetime.now().isoformat() + 'Z'

        # Tr·∫°ng th√°i
        status_elem = soup.select_one('span.label-status')
        if status_elem:
            text = status_elem.text.strip()
            if "Full" in text:
                state = "Ho√†n th√†nh"
            elif "ƒêang c·∫≠p nh·∫≠t" in text:
                state = "ƒêang ti·∫øn h√†nh"
            else:
                state = "Ch∆∞a x√°c ƒë·ªãnh"
        else:
            state = "Ch∆∞a x√°c ƒë·ªãnh"

        # Th·ªÉ lo·∫°i
        genre_elem = soup.select_one('li.li--genres')
        genres = [g.text.strip() for g in genre_elem.find_all('a')] if genre_elem else ["Ch∆∞a x√°c ƒë·ªãnh"]
        category_ids = [category_mapping.get(g, "") for g in genres if g in category_mapping]

        # S·ªë ch∆∞∆°ng
        chap_count = 0
        for li in soup.select('li'):
            if "S·ªë ch∆∞∆°ng" in li.text:
                match = re.search(r"S·ªë ch∆∞∆°ng\s*:\s*(\d+)", li.text)
                if match:
                    chap_count = int(match.group(1))
                break

        # View count
        view_count = 0
        if story['views'] != 'Unknown':
            try:
                view_count = int(story['views'].replace(',', '').replace('.', ''))
            except:
                pass

        return {
            'Name': story['title'],
            'Description': description,
            'Author': story['author'],
            'State': state,
            'ChapCount': chap_count,
            'ImgUrl': img_url,
            'Categories': category_ids,
            'dateUpdate': date_update,
            'ViewCount': view_count,
            'TotalComments': 0
        }

    except requests.RequestException as e:
        print(f'‚ùå L·ªói khi l·∫•y th√¥ng tin t·ª´ {url}: {e}')
        return None

def get_story_details(stories):
    total_stories = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(fetch_story_detail, story) for story in stories]
        for future in as_completed(futures):
            result = future.result()
            if result:
                total_stories.append(result)

    with open('total_stories.json', 'w', encoding='utf-8') as f:
        json.dump(total_stories, f, ensure_ascii=False, indent=2)

    print(f'‚úÖ ƒê√£ l∆∞u {len(total_stories)} truy·ªán v√†o total_stories.json')
    return total_stories

def get_stories(start_page=1, end_page=2):
    all_stories = []

    for page in range(start_page, end_page + 1):
        url = f'{BASE_URL}/danh-sach/truyen-hot' if page == 1 else f'{BASE_URL}/danh-sach/truyen-hot?page={page}'
        print(f'üîç ƒêang l·∫•y d·ªØ li·ªáu t·ª´: {url}')

        try:
            response = requests.get(url, headers=HEADERS)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            for elem in soup.select('.item'):
                title_elem = elem.select_one('h3 a')
                if not title_elem:
                    continue

                story_name = title_elem.text.strip()
                story_url = urljoin(BASE_URL, title_elem['href'])

                author_elem = elem.select_one('.line a[href^="/tac-gia/"]')
                author = author_elem.text.strip() if author_elem else 'Unknown'

                genre_elem = elem.select_one('.line a[href^="/the-loai/"]')
                genre = genre_elem.text.strip() if genre_elem else 'Unknown'

                image_elem = elem.select_one('a.cover img')
                img_url = image_elem['src'] if image_elem else None
                if img_url and not img_url.startswith(('http://', 'https://')):
                    img_url = urljoin(BASE_URL, img_url)

                chapters = 'Unknown'
                views = 'Unknown'
                for span in elem.select('.line span'):
                    if "S·ªë ch∆∞∆°ng :" in span.text:
                        chapters = span.text.split(':')[-1].strip()
                    elif "L∆∞·ª£t xem :" in span.text:
                        views = span.text.split(':')[-1].strip()

                story = {
                    'title': story_name,
                    'url': story_url,
                    'author': author,
                    'genre': genre,
                    'chapters': chapters,
                    'views': views,
                    'image_url': img_url,
                }
                all_stories.append(story)

            time.sleep(1)

        except requests.RequestException as e:
            print(f'‚ùå L·ªói khi l·∫•y trang {url}: {e}')

    return get_story_details(all_stories)

if __name__ == "__main__":
    get_stories(start_page=1, end_page=2)
